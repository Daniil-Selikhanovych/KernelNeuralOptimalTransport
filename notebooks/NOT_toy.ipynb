{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "\n",
    "from src import distributions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.tools import unfreeze, freeze\n",
    "from src import distributions\n",
    "\n",
    "from src.plotters import plot_generated_2D, plot_bar_and_stochastic_2D\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_IDS = [1]\n",
    "\n",
    "T_ITERS = 10\n",
    "D_LR, T_LR = 1e-4, 1e-4\n",
    "\n",
    "ZD = 2\n",
    "Z_STD = 0.1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "Z_SIZE = 4\n",
    "\n",
    "PLOT_INTERVAL = 200\n",
    "MAX_STEPS = 10001\n",
    "SEED = 0x000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 2\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(DEVICE_IDS[0])\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampler = distributions.StandardNormalSampler(dim=DIM)\n",
    "Y_sampler = distributions.StandardNormalScaler(distributions.SwissRollSampler())\n",
    "\n",
    "# distributions.StandardNormalScaler(distributions.SwissRollSampler()) # <-- Swiss Roll\n",
    "# distributions.StandardNormalScaler(distributions.Mix8GaussiansSampler()) # <-- 8 Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_mlp(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "H = 100\n",
    "\n",
    "T = nn.Sequential(\n",
    "    nn.Linear(DIM+ZD, H),\n",
    "    nn.ReLU(True), \n",
    "    nn.Linear(H, H),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(H, H),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(H, DIM)\n",
    ").cuda()\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(DIM, H),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(H, H),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(H, H),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(H, 1)\n",
    ").cuda()\n",
    "\n",
    "T.apply(weights_init_mlp); D.apply(weights_init_mlp)\n",
    "\n",
    "if len(DEVICE_IDS) > 1:\n",
    "    T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
    "    D = nn.DataParallel(D, device_ids=DEVICE_IDS)\n",
    "\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('D params:', np.sum([np.prod(p.shape) for p in D.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_opt = torch.optim.Adam(T.parameters(), lr=T_LR, weight_decay=1e-10)\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR, weight_decay=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for step in tqdm(range(MAX_STEPS)):\n",
    "    # T optimization\n",
    "    unfreeze(T); freeze(D)\n",
    "    for t_iter in range(T_ITERS):\n",
    "        T_opt.zero_grad()\n",
    "        X = X_sampler.sample(BATCH_SIZE).reshape(-1, 1, DIM).repeat(1, Z_SIZE, 1)\n",
    "        with torch.no_grad():\n",
    "            Z = torch.randn(BATCH_SIZE, Z_SIZE, ZD, device='cuda') * Z_STD\n",
    "            XZ = torch.cat([X, Z], dim=2)\n",
    "        T_XZ = T(\n",
    "            XZ.flatten(start_dim=0, end_dim=1)\n",
    "        ).permute(1, 0).reshape(DIM, -1, Z_SIZE).permute(1, 2, 0)\n",
    "\n",
    "        T_loss = F.mse_loss(X[:, 0], T_XZ.mean(dim=1)).mean() - D(\n",
    "            T_XZ.flatten(start_dim=0, end_dim=1)).mean() - T_XZ.var(dim=1).mean() / Z_SIZE\n",
    "        T_loss.backward(); T_opt.step()\n",
    "    \n",
    "    del T_loss, T_XZ, X, Z\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # D optimization\n",
    "    freeze(T); unfreeze(D)\n",
    "\n",
    "    X = X_sampler.sample(BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        Z = torch.randn(BATCH_SIZE, ZD, device='cuda') * Z_STD\n",
    "        XZ = torch.cat([X, Z], dim=1)\n",
    "        T_XZ = T(XZ)\n",
    "    Y = Y_sampler.sample(BATCH_SIZE)\n",
    "\n",
    "    D_opt.zero_grad()\n",
    "    D_loss = D(T_XZ).mean() - D(Y).mean()\n",
    "    D_loss.backward(); D_opt.step()\n",
    "\n",
    "    del D_loss, Y, X, T_XZ, Z, XZ\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    if step % PLOT_INTERVAL == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Step\", step)\n",
    "\n",
    "        fig, axes = plot_generated_2D(X_sampler, Y_sampler, T, ZD, Z_STD)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, axes = plot_bar_and_stochastic_2D(X_sampler, Y_sampler, T, ZD, Z_STD)\n",
    "        plt.show()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
